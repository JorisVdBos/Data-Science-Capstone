---
title: "Data science coursera track final project"
author: "Joris Van den Bossche"
date: "26 september 2016"
output: 
  html_document:
    keep_md: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_knit$set(root.dir = normalizePath('../'))
```

# Global options
global options and the libraries used are found in the Global folder:
```{r}
source("00_Global/libraries.R")
source("00_Global/settings.R")
```


# Reading in data
Functions are defined in this folder for downloading, sampling and loading the data into R.
```{r sourceLoad}
source("01_Load/load.R")
```

## Usage
Downloading the data:
```{r downloadFiles}
if(!file.exists("RawData/final")){
  downloadFiles()
}
```

To load a sample of the files into R, this function was created:
```{r readSample}
readTextsSample(lines = 2, lang = "en_US")
twitterTexts
```

Creating a subfolder "sample" in the "RawData" folder. This may take a few minutes.
```{r createSample, eval = FALSE}
createSampleDataDir(usePercentageOfData, seed, lang = "en_US")
```

Reading the sample data into a corpus is done using this function:
```{r createCorpus, cache = TRUE, results = 'hide'}
if(!exists("corpus") || !exists("tdm"))
  createCorpus(lang = "en_US")
```

# Exploratory Analysis
```{r sourceExplAnalysis}
source("02_ExploratoryAnalysis/Explore.r")
```

Constructing a word freqency table from a term document matrix:
```{r wordFreq}
wordFreq <- wordFreq(tdm)
wordFreq[1:10, ] # The 10 most frequent words
```

Most words only appear once. The frequency is distributed as such:
```{r wordDensity}
density <- wordFreq[, .(count = .N), by = freq]
density[, density := count /sum(density$count)]
density[order(-density)][density > 0.005]
```

Creating the table of n-grams of length 2:
```{r ngram2}
n2grams <- ngramsFromCorpus(corpus, n = 2)
n2gramsTable <- data.table(get.phrasetable(n2grams))
n2gramsTable <- n2gramsTable[freq > 1] #This will filter out the n-grams that were unique 
n2gramsTable[, density := freq / sum(n2gramsTable$freq)]
n2gramsTable[order(-density)][density > 0.002]
```

A histogram of the freqency of the ngrams of length 2:
```{r ngram2hist}
ggplot(data= table) + 
  geom_freqpoly(aes(freq))
```

As expected, most n-grams are quite rare. The same for the n-grams of length 3:
```{r ngram3}
n3grams <- ngramsFromCorpus(corpus, n = 3)
n3gramsTable <- data.table(get.phrasetable(n3grams))
n3gramsTable <- n3gramsTable[freq > 1] #This will filter out the n-grams that were unique 
n3gramsTable[, density := freq / sum(n3gramsTable$freq)]
n3gramsTable[order(-density)][density > 0.002]
```

A histogram of the freqency of the ngrams of length 3:
```{r ngram3hist}
ggplot(data= table) + 
  geom_freqpoly(aes(freq))
```

# Modelling
```{r modelling}
source("03_Modelling/Models.R")
```

This first model looks at the freqency table of the 3-grams and takes the three options that are most frequent. If less than three options are found, it will look at the 2-grams table. If still no options are found, it will look at the most freqent words.
```{r model1Examples}
freqModel(wordFreq, n2gramsTable, n3gramsTable, "then", "you")
freqModel(wordFreq, n2gramsTable, n3gramsTable, "sometimes", "you")
freqModel(wordFreq, n2gramsTable, n3gramsTable, "xnjiqqfqsf", "you")
freqModel(wordFreq, n2gramsTable, n3gramsTable, "xnjiqqfqsf", "nqnjndjiqpdns")
```

